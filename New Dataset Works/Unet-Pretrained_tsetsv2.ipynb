{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgg39RIwQO1T",
        "outputId": "8af91871-9f9e-4b56-fad5-2b2601b15c96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgg6PpJVna2x"
      },
      "outputs": [],
      "source": [
        "# extract data from zip file\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the path to your zip file and the directory where you want to extract it\n",
        "zip_file_path = '/content/drive/MyDrive/465 Project/new_archive.zip'\n",
        "extract_folder_path = '/content/data'\n",
        "\n",
        "os.makedirs(extract_folder_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xz7G45_MVilc"
      },
      "outputs": [],
      "source": [
        "# lib imports\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import cv2\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from skimage.io import imread, imshow\n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQf8GJC3WxBu"
      },
      "outputs": [],
      "source": [
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "IMG_CHANNELS = 3\n",
        "IMG_COUNT = 480"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KB9yUggrZ5u5"
      },
      "outputs": [],
      "source": [
        "# set this between [1, 10] for differently composed datasets\n",
        "# reference: https://arxiv.org/pdf/2307.05911.pdf\n",
        "TRAINING_SET = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRB5U53vS6NK"
      },
      "source": [
        "# 10 Different training sets as mentioned in original paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBIq7T3LOx9W"
      },
      "outputs": [],
      "source": [
        "#@title This cell is for composing the datasets as mentioned in the original paper\n",
        "\n",
        "# Set the target image size\n",
        "target_size = (IMG_WIDTH, IMG_HEIGHT)\n",
        "\n",
        "# Create empty lists to hold the images and masks\n",
        "images = []\n",
        "masks = []\n",
        "\n",
        "image_dir_ms = '/content/data/GRAIN DATA SET/RG'\n",
        "mask_dir_ms = '/content/data/GRAIN DATA SET/RGMask'\n",
        "# vt stands for voronoi tessellation\n",
        "image_dir_vt = '/content/data/GRAIN DATA SET/AG'\n",
        "mask_dir_vt = '/content/data/GRAIN DATA SET/AGMask'\n",
        "\n",
        "image_dir_hed = '/content/data/GRAIN DATA SET/HED_PRE'\n",
        "mask_dir_hed = '/content/data/GRAIN DATA SET/RGMask'\n",
        "\n",
        "image_dir_grad = '/content/data/GRAIN DATA SET/GRAD_PRE'\n",
        "mask_dir_grad = '/content/data/GRAIN DATA SET/RGMask'\n",
        "\n",
        "image_dir_thresh = '/content/data/GRAIN DATA SET/THRESH_PRE'\n",
        "mask_dir_thresh = '/content/data/GRAIN DATA SET/RGMask'\n",
        "\n",
        "if TRAINING_SET==1:\n",
        "    # 100% Manually Segmented\n",
        "    print(\"Selected Training Set 1\")\n",
        "    print(\"Manually Segmented - 100% (480)\")\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    for file in sorted(os.listdir(image_dir_ms)):\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_ms, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "\n",
        "    for file in sorted(os.listdir(mask_dir_ms)):\n",
        "\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_ms, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "\n",
        "elif TRAINING_SET==2:\n",
        "    # 25% Artificial, 75% Manually Segmented (160, 480)\n",
        "    print(\"Selected Training Set 2\")\n",
        "    print(\"Voronoi Tessellation - 25% (160)\")\n",
        "    print(\"Manually Segmented - 75% (480)\")\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    for file in sorted(os.listdir(image_dir_ms)):\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_ms, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "\n",
        "    for file in sorted(os.listdir(mask_dir_ms)):\n",
        "\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_ms, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "\n",
        "    cnt = 0\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    for file in sorted(os.listdir(image_dir_vt)):\n",
        "        if cnt==160:\n",
        "            break\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_vt, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "        cnt+=1\n",
        "\n",
        "    cnt=0\n",
        "    for file in sorted(os.listdir(mask_dir_vt)):\n",
        "        if cnt==160:\n",
        "            break\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_vt, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "        cnt+=1\n",
        "\n",
        "elif TRAINING_SET==3:\n",
        "    # 50% Artificial, 50% Manually Segmented (480, 480)\n",
        "    print(\"Selected Training Set 3\")\n",
        "    print(\"Voronoi Tessellation - 50% (480)\")\n",
        "    print(\"Manually Segmented - 50% (480)\")\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    for file in sorted(os.listdir(image_dir_ms)):\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_ms, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "\n",
        "    for file in sorted(os.listdir(mask_dir_ms)):\n",
        "\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_ms, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "\n",
        "    cnt = 0\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    for file in sorted(os.listdir(image_dir_vt)):\n",
        "        if cnt==480:\n",
        "            break\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_vt, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "        cnt+=1\n",
        "\n",
        "    cnt=0\n",
        "    for file in sorted(os.listdir(mask_dir_vt)):\n",
        "        if cnt==480:\n",
        "            break\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_vt, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "        cnt+=1\n",
        "\n",
        "elif TRAINING_SET==4:\n",
        "    # 75% Artificial, 25% Manually Segmented (800, 266)\n",
        "    print(\"Selected Training Set 4\")\n",
        "    print(\"Voronoi Tessellation - 75% (800)\")\n",
        "    print(\"Manually Segmented - 25% (266)\")\n",
        "    cnt = 0\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    for file in sorted(os.listdir(image_dir_ms)):\n",
        "        if cnt==266:\n",
        "            break\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_ms, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "        cnt+=1\n",
        "\n",
        "    cnt=0\n",
        "    for file in sorted(os.listdir(mask_dir_ms)):\n",
        "        if cnt==266:\n",
        "            break\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_ms, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "        cnt+=1\n",
        "\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    for file in sorted(os.listdir(image_dir_vt)):\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_vt, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "\n",
        "    for file in sorted(os.listdir(mask_dir_vt)):\n",
        "\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_vt, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "\n",
        "elif TRAINING_SET==5:\n",
        "    # 100% Artificial (800)\n",
        "    print(\"Selected Training Set 5\")\n",
        "    print(\"Voronoi Tessellation - 100% (800)\")\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    for file in sorted(os.listdir(image_dir_vt)):\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_vt, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "\n",
        "    for file in sorted(os.listdir(mask_dir_vt)):\n",
        "\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_vt, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "\n",
        "elif TRAINING_SET==6:\n",
        "    # 50% HED Preprocessed, 50% Manually Segmented (240, 240)\n",
        "    print(\"Selected Training Set 6\")\n",
        "    print(\"HED Preprocessed - 50% (240)\")\n",
        "    print(\"Manually Segmented - 50% (240)\")\n",
        "    cnt = 0\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    for file in sorted(os.listdir(image_dir_ms)):\n",
        "        if cnt==240:\n",
        "            break\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_ms, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "        cnt+=1\n",
        "\n",
        "    cnt=0\n",
        "    for file in sorted(os.listdir(mask_dir_ms)):\n",
        "        if cnt==240:\n",
        "            break\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_ms, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "        cnt+=1\n",
        "\n",
        "    cnt = 0\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    for file in sorted(os.listdir(image_dir_hed)):\n",
        "        if cnt<240:\n",
        "            cnt+=1\n",
        "            continue\n",
        "        if cnt==480:\n",
        "            break\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_hed, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "        cnt+=1\n",
        "\n",
        "    cnt=0\n",
        "    for file in sorted(os.listdir(mask_dir_hed)):\n",
        "        if cnt<240:\n",
        "            cnt+=1\n",
        "            continue\n",
        "        if cnt==480:\n",
        "            break\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_hed, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "        cnt+=1\n",
        "\n",
        "elif TRAINING_SET==7:\n",
        "    # 50% Manually Segmented, 25% Artificial, 25% HED Preprocessed(240, 120, 120)\n",
        "    print(\"Selected Training Set 7\")\n",
        "    print(\"Manually Segmented - 50% (240)\")\n",
        "    print(\"Voronoi Tessellation - 25% (120)\")\n",
        "    print(\"HED Preprocessed - 25% (120)\")\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    cnt=0\n",
        "    for file in sorted(os.listdir(image_dir_ms)):\n",
        "        if cnt==240:\n",
        "            break\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_ms, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "        cnt+=1\n",
        "\n",
        "    cnt=0\n",
        "    for file in sorted(os.listdir(mask_dir_ms)):\n",
        "        if cnt==240:\n",
        "            break\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_ms, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "        cnt+=1\n",
        "\n",
        "\n",
        "    cnt = 0\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    for file in sorted(os.listdir(image_dir_vt)):\n",
        "        if cnt==120:\n",
        "            break\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_vt, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "        cnt+=1\n",
        "\n",
        "    cnt=0\n",
        "    for file in sorted(os.listdir(mask_dir_vt)):\n",
        "        if cnt==120:\n",
        "            break\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_vt, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "        cnt+=1\n",
        "\n",
        "\n",
        "    cnt = 0\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    for file in sorted(os.listdir(image_dir_hed)):\n",
        "        if cnt<240:\n",
        "            cnt+=1\n",
        "            continue\n",
        "        if cnt==360:\n",
        "            break\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_hed, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "        cnt+=1\n",
        "\n",
        "    cnt=0\n",
        "    for file in sorted(os.listdir(mask_dir_hed)):\n",
        "        if cnt<240:\n",
        "            cnt+=1\n",
        "            continue\n",
        "        if cnt==360:\n",
        "            break\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_hed, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "        cnt+=1\n",
        "\n",
        "\n",
        "elif TRAINING_SET==8:\n",
        "    # 50% Manually Segmented, 25% Artificial, 25% GRAD Preprocessed(240, 120, 120)\n",
        "    print(\"Selected Training Set 8\")\n",
        "    print(\"Manually Segmented - 50% (240)\")\n",
        "    print(\"Voronoi Tessellation - 25% (120)\")\n",
        "    print(\"GRAD Preprocessed - 25% (120)\")\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    cnt=0\n",
        "    for file in sorted(os.listdir(image_dir_ms)):\n",
        "        if cnt==240:\n",
        "            break\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_ms, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "        cnt+=1\n",
        "\n",
        "    cnt=0\n",
        "    for file in sorted(os.listdir(mask_dir_ms)):\n",
        "        if cnt==240:\n",
        "            break\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_ms, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "        cnt+=1\n",
        "\n",
        "\n",
        "    cnt = 0\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    for file in sorted(os.listdir(image_dir_vt)):\n",
        "        if cnt==120:\n",
        "            break\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_vt, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "        cnt+=1\n",
        "\n",
        "    cnt=0\n",
        "    for file in sorted(os.listdir(mask_dir_vt)):\n",
        "        if cnt==120:\n",
        "            break\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_vt, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "        cnt+=1\n",
        "\n",
        "\n",
        "    cnt = 0\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    for file in sorted(os.listdir(image_dir_grad)):\n",
        "        if cnt<240:\n",
        "            cnt+=1\n",
        "            continue\n",
        "        if cnt==360:\n",
        "            break\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_grad, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "        cnt+=1\n",
        "\n",
        "    cnt=0\n",
        "    for file in sorted(os.listdir(mask_dir_grad)):\n",
        "        if cnt<240:\n",
        "            cnt+=1\n",
        "            continue\n",
        "        if cnt==360:\n",
        "            break\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_grad, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "        cnt+=1\n",
        "\n",
        "elif TRAINING_SET==9:\n",
        "    # 50% Manually Segmented, 25% Artificial, 25% THRESHOLD Preprocessed(240, 120, 120)\n",
        "    print(\"Selected Training Set 9\")\n",
        "    print(\"Manually Segmented - 50% (240)\")\n",
        "    print(\"Voronoi Tessellation - 25% (120)\")\n",
        "    print(\"THRESHOLD Preprocessed - 25% (120)\")\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    cnt=0\n",
        "    for file in sorted(os.listdir(image_dir_ms)):\n",
        "        if cnt==240:\n",
        "            break\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_ms, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "        cnt+=1\n",
        "\n",
        "    cnt=0\n",
        "    for file in sorted(os.listdir(mask_dir_ms)):\n",
        "        if cnt==240:\n",
        "            break\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_ms, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "        cnt+=1\n",
        "\n",
        "\n",
        "    cnt = 0\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    for file in sorted(os.listdir(image_dir_vt)):\n",
        "        if cnt==120:\n",
        "            break\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_vt, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "        cnt+=1\n",
        "\n",
        "    cnt=0\n",
        "    for file in sorted(os.listdir(mask_dir_vt)):\n",
        "        if cnt==120:\n",
        "            break\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_vt, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "        cnt+=1\n",
        "\n",
        "\n",
        "    cnt = 0\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    for file in sorted(os.listdir(image_dir_thresh)):\n",
        "        if cnt<240:\n",
        "            cnt+=1\n",
        "            continue\n",
        "        if cnt==360:\n",
        "            break\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_thresh, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "        cnt+=1\n",
        "\n",
        "    cnt=0\n",
        "    for file in sorted(os.listdir(mask_dir_thresh)):\n",
        "        if cnt<240:\n",
        "            cnt+=1\n",
        "            continue\n",
        "        if cnt==360:\n",
        "            break\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_thresh, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "        cnt+=1\n",
        "\n",
        "elif TRAINING_SET==10:\n",
        "    # 50% Manually Segmented, 16.67% HED Prep., 16.67% GRAD Prep.,16.67% THRESHOLD Prep.(240, 80, 80, 80)\n",
        "    print(\"Selected Training Set 10\")\n",
        "    print(\"Manually Segmented - 50% (240)\")\n",
        "    print(\"HED Preprocessed - 16.67% (80)\")\n",
        "    print(\"GRAD Preprocessed - 16.67% (80)\")\n",
        "    print(\"THRESHOLD Preprocessed - 16.67% (80)\")\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    cnt=0\n",
        "    for file in sorted(os.listdir(image_dir_ms)):\n",
        "        if cnt==240:\n",
        "            break\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_ms, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "        cnt+=1\n",
        "\n",
        "    cnt=0\n",
        "    for file in sorted(os.listdir(mask_dir_ms)):\n",
        "        if cnt==240:\n",
        "            break\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_ms, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "        cnt+=1\n",
        "\n",
        "    cnt = 0\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    for file in sorted(os.listdir(image_dir_hed)):\n",
        "        if cnt<240:\n",
        "            cnt+=1\n",
        "            continue\n",
        "        if cnt==320:\n",
        "            break\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_hed, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "        cnt+=1\n",
        "\n",
        "    cnt=0\n",
        "    for file in sorted(os.listdir(mask_dir_hed)):\n",
        "        if cnt<240:\n",
        "            cnt+=1\n",
        "            continue\n",
        "        if cnt==320:\n",
        "            break\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_hed, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "        cnt+=1\n",
        "\n",
        "    cnt = 0\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    for file in sorted(os.listdir(image_dir_grad)):\n",
        "        if cnt<320:\n",
        "            cnt+=1\n",
        "            continue\n",
        "        if cnt==400:\n",
        "            break\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_grad, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "        cnt+=1\n",
        "\n",
        "    cnt=0\n",
        "    for file in sorted(os.listdir(mask_dir_grad)):\n",
        "        if cnt<320:\n",
        "            cnt+=1\n",
        "            continue\n",
        "        if cnt==400:\n",
        "            break\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_grad, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "        cnt+=1\n",
        "\n",
        "    cnt = 0\n",
        "    # Iterate through the directories and load the images and masks\n",
        "    for file in sorted(os.listdir(image_dir_thresh)):\n",
        "        if cnt<400:\n",
        "            cnt+=1\n",
        "            continue\n",
        "        if cnt==480:\n",
        "            break\n",
        "        # Load the image and resize to the target size\n",
        "        img = cv2.imread(os.path.join(image_dir_thresh, file))\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Append the resized image to the list of images\n",
        "        images.append(img)\n",
        "        cnt+=1\n",
        "\n",
        "    cnt=0\n",
        "    for file in sorted(os.listdir(mask_dir_thresh)):\n",
        "        if cnt<400:\n",
        "            cnt+=1\n",
        "            continue\n",
        "        if cnt==480:\n",
        "            break\n",
        "        # Load the corresponding mask and resize to the target size\n",
        "        mask = cv2.imread(os.path.join(mask_dir_thresh, file))\n",
        "        mask = cv2.resize(mask, target_size)\n",
        "\n",
        "        # Append the resized mask to the list of masks\n",
        "        masks.append(mask)\n",
        "        cnt+=1\n",
        "\n",
        "\n",
        "\n",
        "IMG_COUNT = len(images)\n",
        "print(f\"image count : {IMG_COUNT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGhwVJFSW1Ug"
      },
      "outputs": [],
      "source": [
        "# # Set the directories containing the images and masks\n",
        "# image_dir = '/content/drive/MyDrive/465 Project/ArtificialDataset/GRAIN DATA SET/AG'\n",
        "# mask_dir = '/content/drive/MyDrive/465 Project/ArtificialDataset/GRAIN DATA SET/AGMask'\n",
        "\n",
        "# # Set the target image size\n",
        "# target_size = (IMG_WIDTH, IMG_HEIGHT)\n",
        "\n",
        "# # Create empty lists to hold the images and masks\n",
        "# images = []\n",
        "# masks = []\n",
        "\n",
        "# # Iterate through the directories and load the images and masks\n",
        "# for file in sorted(os.listdir(image_dir)):\n",
        "#     # Load the image and resize to the target size\n",
        "#     img = cv2.imread(os.path.join(image_dir, file))\n",
        "#     img = cv2.resize(img, target_size)\n",
        "\n",
        "#     # Append the resized image to the list of images\n",
        "#     images.append(img)\n",
        "\n",
        "# for file in sorted(os.listdir(mask_dir)):\n",
        "\n",
        "#     # Load the corresponding mask and resize to the target size\n",
        "#     #mask_file = file.replace('.jpg', '.png')\n",
        "#     mask = cv2.imread(os.path.join(mask_dir, file))\n",
        "#     mask = cv2.resize(mask, target_size)\n",
        "\n",
        "#     # Append the resized mask to the list of masks\n",
        "#     masks.append(mask)\n",
        "\n",
        "# IMG_COUNT = len(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihXq1aKcZuVQ"
      },
      "source": [
        "# Example Image and it's Ground Truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpDzy0FQYNA8"
      },
      "outputs": [],
      "source": [
        "# example\n",
        "image_x = random.randint(0, IMG_COUNT)\n",
        "image_x\n",
        "imshow(images[image_x])\n",
        "plt.show()\n",
        "imshow(masks[image_x])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IawsoiuQBxU3"
      },
      "source": [
        "# dataset splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiRlzdUuc6ca"
      },
      "outputs": [],
      "source": [
        "# create the X and Y (input and output)\n",
        "\n",
        "X_train = np.array(images)\n",
        "Y_train = np.array(masks)\n",
        "print(X_train.shape, Y_train.shape)\n",
        "# change the Y to a boolean\n",
        "Y_train = np.where(Y_train > 245, True, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I303u1Z3OZZT"
      },
      "outputs": [],
      "source": [
        "# careful to run this cell only once (otherwise shape changes)\n",
        "X_train = np.transpose(X_train, (0, 3, 1 , 2))\n",
        "Y_train = np.transpose(Y_train, (0, 3, 1 , 2))\n",
        "print(X_train.shape, Y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtXB7ItEc-J9"
      },
      "outputs": [],
      "source": [
        "#convert the boolean where it insion s true (any of the 3 channels) to a (336, 128, 128, 1)\n",
        "#basically reduce the 3 channel dimeRGB to just one boolean value\n",
        "\n",
        "Y_t= np.any(Y_train, axis=1)\n",
        "print(Y_t.shape)\n",
        "Y_t = Y_t.reshape(IMG_COUNT, 1, 128, 128)\n",
        "print(Y_t.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKN9aaGqmgT7"
      },
      "outputs": [],
      "source": [
        "# prompt: Make Dataset from data\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class GrainDataset(Dataset):\n",
        "    def __init__(self, images, masks):\n",
        "        self.images = images\n",
        "        self.masks = masks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = torch.from_numpy(self.images[idx]).float()\n",
        "        mask = torch.from_numpy(self.masks[idx]).float()\n",
        "        return image, mask\n",
        "\n",
        "# Create the dataset\n",
        "dataset = GrainDataset(X_train, Y_t)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S89-QaPhnkf2"
      },
      "outputs": [],
      "source": [
        "# prompt: create train and validation dataset\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# Split the dataset into train and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U segmentation-models-pytorch"
      ],
      "metadata": {
        "id": "BPZgWh-Oalve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U git+https://github.com/qubvel/segmentation_models.pytorch"
      ],
      "metadata": {
        "id": "cuPuJLbMaogN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "model = smp.Unet(\n",
        "    encoder_name=\"resnet50\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
        "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
        "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
        "    classes=1,                      # model output channels (number of classes in your dataset)\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "zBw5-Ld2bAEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1ze9oRSeFL9"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = float('inf')\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss):\n",
        "        \"\"\"Saves model when validation loss decrease.\"\"\"\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model...')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Define DataLoader for training and validation sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Define callbacks\n",
        "class ModelCheckpoint:\n",
        "    def __init__(self, checkpoint_path):\n",
        "        self.checkpoint_path = checkpoint_path\n",
        "        self.best_loss = float('inf')\n",
        "\n",
        "    def __call__(self, model, epoch, train_loss, val_loss):\n",
        "        if val_loss < self.best_loss:\n",
        "            torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'train_loss' : train_loss,\n",
        "            'val_loss': val_loss,\n",
        "            }, self.checkpoint_path)\n",
        "            print(\"Checkpoint saved\")\n",
        "            self.best_loss = val_loss\n",
        "\n",
        "from datetime import datetime\n",
        "checkpoint_path = f'/content/drive/MyDrive/465 Project/Unet Checkpoints/TrainingSet{TRAINING_SET}_{datetime.now().strftime(\"%d%m%Y %H:%M:%S\")}.pt'\n",
        "# print(checkpoint_path)\n",
        "# os.makedirs(\"/content/drive/MyDrive/465 Project\", exist_ok=True) # already exists\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "checkpointer = ModelCheckpoint(checkpoint_path)\n",
        "\n",
        "# Initialize the early stopper\n",
        "early_stopper = EarlyStopping(patience=10, verbose=True)\n",
        "\n",
        "num_epochs = 40\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        if device == 'cuda':\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            if device == 'cuda':\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    # Update checkpoint if improved\n",
        "    #checkpointer(model, epoch, train_loss, val_loss)\n",
        "\n",
        "    # Check for early stopping\n",
        "    early_stopper(val_loss)\n",
        "    if early_stopper.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6rODygYDo1i"
      },
      "outputs": [],
      "source": [
        "# current_checkpoint = torch.load(checkpoint_path)\n",
        "# model.load_state_dict(current_checkpoint['model_state_dict'])\n",
        "# model.eval()\n",
        "# print(current_checkpoint['train_loss'],current_checkpoint['val_loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqMBAVWcLDln"
      },
      "outputs": [],
      "source": [
        "def display_results(image, ground_truth_mask, predicted_mask):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # Display input image\n",
        "    axes[0].imshow(np.transpose(image, (1, 2, 0)))\n",
        "    axes[0].set_title('Input Image')\n",
        "\n",
        "    # Display ground truth mask\n",
        "    axes[1].imshow(np.squeeze(ground_truth_mask))\n",
        "    axes[1].set_title('Ground Truth Mask')\n",
        "\n",
        "    # Display predicted mask\n",
        "    axes[2].imshow(np.squeeze(predicted_mask))\n",
        "    axes[2].set_title('Predicted Mask')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IraOKMDpCH1Q"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDqKhTqXCMuX"
      },
      "outputs": [],
      "source": [
        "# from torchmetrics.functional import dice\n",
        "# from torchmetrics.functional import jaccard_index\n",
        "\n",
        "# total_jaccard_score = 0.0\n",
        "# total_dice_score = 0.0\n",
        "# total_num_samples = 0\n",
        "\n",
        "# for images, masks in val_loader:\n",
        "#     batch_size = images.size(0)  # Get the actual batch size\n",
        "#     total_num_samples += batch_size\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         images = images.to(device)\n",
        "#         masks = masks.to(device)\n",
        "\n",
        "#         # Forward pass\n",
        "#         preds = model(images)\n",
        "\n",
        "#         for ind in range(len(preds)):\n",
        "#             pred_tensor = (preds[ind] > 0.5).float()\n",
        "#             gt_tensor = masks[ind].to(torch.int64)  # Convert mask tensor to integer tensor\n",
        "\n",
        "#             dice_score = dice(pred_tensor, gt_tensor)\n",
        "#             total_dice_score += dice_score.item()\n",
        "\n",
        "#             jaccard_score = jaccard_index(pred_tensor, gt_tensor, task='binary')\n",
        "#             total_jaccard_score += jaccard_score.item()\n",
        "\n",
        "# average_dice_score = total_dice_score / total_num_samples\n",
        "# average_jaccard_score = total_jaccard_score / total_num_samples\n",
        "\n",
        "\n",
        "# print(f\"Total Number samples: {total_num_samples}\")\n",
        "# print(f\"Average Dice Score: {average_dice_score}\")\n",
        "# print(f\"Average Jaccard Index: {average_jaccard_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD5d7iHHpWeW"
      },
      "outputs": [],
      "source": [
        "# using the while RG dataset as validation dataset to calculate dice score\n",
        "rg_image_dir = '/content/data/GRAIN DATA SET/RG'\n",
        "rg_mask_dir = '/content/data/GRAIN DATA SET/RGMask'\n",
        "\n",
        "images = []\n",
        "masks = []\n",
        "\n",
        "# Iterate through the directories and load the images and masks\n",
        "for file in sorted(os.listdir(rg_image_dir)):\n",
        "    # Load the image and resize to the target size\n",
        "    img = cv2.imread(os.path.join(rg_image_dir, file))\n",
        "    img = cv2.resize(img, target_size)\n",
        "\n",
        "    # Append the resized image to the list of images\n",
        "    images.append(img)\n",
        "\n",
        "for file in sorted(os.listdir(rg_mask_dir)):\n",
        "\n",
        "    # Load the corresponding mask and resize to the target size\n",
        "    mask = cv2.imread(os.path.join(rg_mask_dir, file))\n",
        "    mask = cv2.resize(mask, target_size)\n",
        "\n",
        "    # Append the resized mask to the list of masks\n",
        "    masks.append(mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAjtjdmMqU23"
      },
      "outputs": [],
      "source": [
        "# create the X and Y (input and output)\n",
        "\n",
        "X_rg = np.array(images)\n",
        "Y_rg = np.array(masks)\n",
        "print(X_rg.shape, Y_rg.shape)\n",
        "# change the Y to a boolean\n",
        "Y_rg = np.where(Y_rg > 245, True, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUxhqucVqU23"
      },
      "outputs": [],
      "source": [
        "# careful to run this cell only once (otherwise shape changes)\n",
        "X_rg = np.transpose(X_rg, (0, 3, 1 , 2))\n",
        "Y_rg = np.transpose(Y_rg, (0, 3, 1 , 2))\n",
        "print(X_rg.shape, Y_rg.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32OJV-g7qU23"
      },
      "outputs": [],
      "source": [
        "#convert the boolean where it insion s true (any of the 3 channels) to a (336, 128, 128, 1)\n",
        "#basically reduce the 3 channel dimeRGB to just one boolean value\n",
        "\n",
        "Y_t= np.any(Y_rg, axis=1)\n",
        "print(Y_t.shape)\n",
        "Y_t = Y_t.reshape(480, 1, 128, 128)\n",
        "print(Y_t.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsDjHWZf2PlR"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf1h1FTTqU23"
      },
      "outputs": [],
      "source": [
        "dataset = GrainDataset(X_rg, Y_t)\n",
        "rg_val_loader = DataLoader(dataset,batch_size=16, shuffle=False)\n",
        "\n",
        "from torchmetrics.functional import dice\n",
        "from torchmetrics.functional import jaccard_index\n",
        "\n",
        "total_jaccard_score = 0.0\n",
        "total_dice_score = 0.0\n",
        "total_num_samples = 0\n",
        "\n",
        "for images, masks in rg_val_loader:\n",
        "    batch_size = images.size(0)  # Get the actual batch size\n",
        "    total_num_samples += batch_size\n",
        "\n",
        "    with torch.no_grad():\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        preds = model(images)\n",
        "\n",
        "        for ind in range(len(preds)):\n",
        "            pred_tensor = (preds[ind] > 0.5).float()\n",
        "            gt_tensor = masks[ind].to(torch.int64)  # Convert mask tensor to integer tensor\n",
        "\n",
        "            dice_score = dice(pred_tensor, gt_tensor)\n",
        "            total_dice_score += dice_score.item()\n",
        "\n",
        "            jaccard_score = jaccard_index(pred_tensor, gt_tensor, task='binary')\n",
        "            total_jaccard_score += jaccard_score.item()\n",
        "\n",
        "average_dice_score = total_dice_score / total_num_samples\n",
        "average_jaccard_score = total_jaccard_score / total_num_samples\n",
        "\n",
        "print(f\"Training set: {TRAINING_SET}\")\n",
        "print(f\"Total Number samples: {total_num_samples}\")\n",
        "print(f\"Average Dice Score: {average_dice_score}\")\n",
        "print(f\"Average Jaccard Index: {average_jaccard_score}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "dRB5U53vS6NK",
        "ihXq1aKcZuVQ",
        "7FVZXwpEBf4H"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}